# AI UA - –õ–æ–∫–∞–ª—å–Ω–∞ AI —Å–∏—Å—Ç–µ–º–∞ –Ω–∞ –±–∞–∑—ñ MamayLM-Gemma-3-12B

–¶–µ production-ready –ª–æ–∫–∞–ª—å–Ω–∞ AI —Å–∏—Å—Ç–µ–º–∞ —è–∫ –∑–∞–º—ñ–Ω–∞ Google Gemini API –∑ –ø–æ–≤–Ω–æ—é –ø—ñ–¥—Ç—Ä–∏–º–∫–æ—é —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—ó –º–æ–≤–∏.

## –û—Å–æ–±–ª–∏–≤–æ—Å—Ç—ñ

- üá∫üá¶ **–£–∫—Ä–∞—ó–Ω—Å—å–∫–∞ –º–æ–¥–µ–ª—å**: MamayLM-Gemma-3-12B-IT (Q5_K_M –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü—ñ—è)
- üöÄ **–®–≤–∏–¥–∫–∞ —Ä–æ–±–æ—Ç–∞**: –û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–æ –¥–ª—è CPU inference –Ω–∞ –±–∞–≥–∞—Ç–æ—è–¥–µ—Ä–Ω–∏—Ö –ø—Ä–æ—Ü–µ—Å–æ—Ä–∞—Ö
- üîÑ **Concurrent processing**: –ü—ñ–¥—Ç—Ä–∏–º–∫–∞ 2+ –æ–¥–Ω–æ—á–∞—Å–Ω–∏—Ö –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤
- üìù **–í–µ–ª–∏–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç**: –î–æ 128K —Ç–æ–∫–µ–Ω—ñ–≤
- üîå **Gemini-compatible API**: –ú—ñ–Ω—ñ–º–∞–ª—å–Ω—ñ –∑–º—ñ–Ω–∏ –≤ —ñ—Å–Ω—É—é—á–æ–º—É –∫–æ–¥—ñ
- üìä **–ú–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥**: Prometheus –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è production
- üê≥ **Docker**: –ü–æ–≤–Ω—ñ—Å—Ç—é –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∏–∑–æ–≤–∞–Ω–µ —Ä—ñ—à–µ–Ω–Ω—è

## –ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Your App       ‚îÇ
‚îÇ  (Node.js/TS)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ HTTP/REST
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  AI API Service (FastAPI)       ‚îÇ
‚îÇ  - /generateContent              ‚îÇ
‚îÇ  - /generateContentStream        ‚îÇ
‚îÇ  - /embedContent                 ‚îÇ
‚îÇ  - /metrics                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ            ‚îÇ
    ‚ñº            ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇLlamaCPP‚îÇ  ‚îÇ Embeddings   ‚îÇ
‚îÇ(GGUF)  ‚îÇ  ‚îÇ Service      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## –®–≤–∏–¥–∫–∏–π —Å—Ç–∞—Ä—Ç

### 1. –ö–ª–æ–Ω—É–≤–∞–Ω–Ω—è —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä—ñ—é

```bash
git clone <your-repo-url> ai_ua
cd ai_ua
```

### 2. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ

```bash
# –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è GGUF –º–æ–¥–µ–ª—ñ (Q5_K_M, ~8.23GB)
./scripts/download_model.sh
```

### 3. –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó

```bash
cp .env.example .env
# –í—ñ–¥—Ä–µ–¥–∞–≥—É–π—Ç–µ .env –∑–∞ –ø–æ—Ç—Ä–µ–±–∏
```

### 4. –ó–∞–ø—É—Å–∫ —á–µ—Ä–µ–∑ Docker Compose

```bash
docker-compose up --build
```

API –±—É–¥–µ –¥–æ—Å—Ç—É–ø–Ω–µ –Ω–∞ `http://localhost:8000`

## –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è

### –ü—Ä–∏–∫–ª–∞–¥ –∑ Node.js Client SDK

```typescript
import { LocalGenerativeAI } from '@ai-ua/client';

// –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è (–∑–∞–º—ñ—Å—Ç—å GoogleGenerativeAI)
const genAI = new LocalGenerativeAI({
  apiUrl: 'http://localhost:8000'
});

const model = genAI.getGenerativeModel({
  model: 'mamay-gemma-3-12b'
});

// –°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—è
const result = await model.generateContent({
  contents: [{
    role: 'user',
    parts: [{ text: '–ü—Ä–∏–≤—ñ—Ç! –Ø–∫ —Å–ø—Ä–∞–≤–∏?' }]
  }],
  generationConfig: {
    temperature: 0.3,
    maxOutputTokens: 8192
  }
});

console.log(result.response.text());

// Streaming
const streamResult = await model.generateContentStream({
  contents: [{ role: 'user', parts: [{ text: '–†–æ–∑–∫–∞–∂–∏ –ø—Ä–æ –ö–∏—ó–≤' }] }]
});

for await (const chunk of streamResult.stream) {
  process.stdout.write(chunk.text());
}

// Embeddings
const embeddingModel = genAI.getGenerativeModel({
  model: 'text-embedding-multilingual'
});

const embedding = await embeddingModel.embedContent('–£–∫—Ä–∞—ó–Ω—Å—å–∫–∏–π —Ç–µ–∫—Å—Ç');
console.log(embedding.embedding.values); // number[] (768 dimensions)
```

### –ü—Ä—è–º—ñ HTTP –∑–∞–ø–∏—Ç–∏

```bash
# –°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—è
curl -X POST http://localhost:8000/v1/models/mamay-gemma-3-12b/generateContent \
  -H "Content-Type: application/json" \
  -d '{
    "contents": [{
      "role": "user",
      "parts": [{"text": "–ü—Ä–∏–≤—ñ—Ç!"}]
    }],
    "generationConfig": {
      "temperature": 0.3,
      "maxOutputTokens": 1024
    }
  }'

# Embeddings
curl -X POST http://localhost:8000/v1/models/text-embedding-multilingual/embedContent \
  -H "Content-Type: application/json" \
  -d '{"content": "–£–∫—Ä–∞—ó–Ω—Å—å–∫–∏–π —Ç–µ–∫—Å—Ç –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü—ñ—ó"}'
```

## Deployment –Ω–∞ —Å–µ—Ä–≤–µ—Ä

### 1. –ù–∞ –ª–æ–∫–∞–ª—å–Ω—ñ–π –º–∞—à–∏–Ω—ñ

```bash
# –¢–µ—Å—Ç—É—î–º–æ –ª–æ–∫–∞–ª—å–Ω–æ
docker-compose up --build

# –ö–æ–º–º—ñ—Ç–∏–º–æ –∑–º—ñ–Ω–∏
git add .
git commit -m "Initial AI UA setup"
git push origin main
```

### 2. –ù–∞ —Å–µ—Ä–≤–µ—Ä—ñ (Ubuntu 22.04)

```bash
# SSH –Ω–∞ —Å–µ—Ä–≤–µ—Ä
ssh vlad_b@ai

# –ö–ª–æ–Ω—É—î–º–æ —Ä–µ–ø–æ
cd /opt  # –∞–±–æ —ñ–Ω—à–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—è
git clone <your-repo-url> ai_ua
cd ai_ua

# –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –º–æ–¥–µ–ª—å
./scripts/download_model.sh

# –ù–∞–ª–∞—à—Ç–æ–≤—É—î–º–æ .env
cp .env.example .env
nano .env  # –≤—ñ–¥—Ä–µ–¥–∞–≥—É–≤–∞—Ç–∏ –∑–∞ –ø–æ—Ç—Ä–µ–±–∏

# –ó–∞–ø—É—Å–∫–∞—î–º–æ
docker-compose up -d --build

# –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –ª–æ–≥–∏
docker-compose logs -f

# –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Å—Ç–∞—Ç—É—Å
curl http://localhost:8000/health
```

## –ú–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥

### Prometheus –º–µ—Ç—Ä–∏–∫–∏

–î–æ—Å—Ç—É–ø–Ω—ñ –Ω–∞ `http://localhost:8000/metrics`:

- `inference_latency_seconds` - —á–∞—Å –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó
- `tokens_per_second` - —à–≤–∏–¥–∫—ñ—Å—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó
- `active_requests` - –∞–∫—Ç–∏–≤–Ω—ñ –∑–∞–ø–∏—Ç–∏
- `queue_size` - —Ä–æ–∑–º—ñ—Ä —á–µ—Ä–≥–∏
- `model_memory_bytes` - –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –ø–∞–º'—è—Ç—ñ

### Health Check

```bash
curl http://localhost:8000/health
# {"status": "healthy", "model_loaded": true, "gpu": false}
```

## –°–∏—Å—Ç–µ–º–Ω—ñ –≤–∏–º–æ–≥–∏

### –ú—ñ–Ω—ñ–º–∞–ª—å–Ω—ñ
- CPU: 8 —è–¥–µ—Ä
- RAM: 16GB
- –î–∏—Å–∫: 20GB –≤—ñ–ª—å–Ω–æ–≥–æ –º—ñ—Å—Ü—è

### –†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω—ñ (–¥–ª—è production)
- CPU: 16+ —è–¥–µ—Ä (–Ω–∞—à —Å–µ—Ä–≤–µ—Ä: 2x Xeon E5-2697A v4, 64 threads)
- RAM: 32GB+ (–Ω–∞—à —Å–µ—Ä–≤–µ—Ä: 125GB)
- –î–∏—Å–∫: 50GB+ SSD
- –ú–µ—Ä–µ–∂–∞: 1Gbps+

## –ü—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å

–ù–∞ —Å–µ—Ä–≤–µ—Ä—ñ HP ProLiant DL360 Gen9 (2x Xeon E5-2697A v4):

- **First token latency**: ~100-200ms
- **Throughput**: ~30-50 tokens/s (–Ω–∞ –∑–∞–ø–∏—Ç)
- **Concurrent users**: 2-4 –±–µ–∑ —á–µ—Ä–≥–∏, –±—ñ–ª—å—à–µ –∑ —á–µ—Ä–≥–æ—é
- **RAM usage**: ~12-15GB (–º–æ–¥–µ–ª—å + –∫–æ–Ω—Ç–µ–∫—Å—Ç)
- **CPU usage**: ~30-50% –ø—Ä–∏ –∞–∫—Ç–∏–≤–Ω—ñ–π –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó

## –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç—É

```
ai_ua/
‚îú‚îÄ‚îÄ backend/              # FastAPI —Å–µ—Ä–≤—ñ—Å
‚îú‚îÄ‚îÄ embeddings-service/   # –û–∫—Ä–µ–º–∏–π —Å–µ—Ä–≤—ñ—Å –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü—ñ—ó
‚îú‚îÄ‚îÄ client-sdk/           # Node.js/TypeScript SDK
‚îú‚îÄ‚îÄ scripts/              # –£—Ç–∏–ª—ñ—Ç–Ω—ñ —Å–∫—Ä–∏–ø—Ç–∏
‚îú‚îÄ‚îÄ docker-compose.yml
‚îî‚îÄ‚îÄ README.md
```

## Troubleshooting

### –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≤–∞–Ω—Ç–∞–∂—É—î—Ç—å—Å—è

```bash
# –ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ –Ω–∞—è–≤–Ω—ñ—Å—Ç—å —Ñ–∞–π–ª—É
ls -lh backend/models/

# –ü–µ—Ä–µ–∑–∞–≤–∞–Ω—Ç–∞–∂—Ç–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä
docker-compose restart api
```

### –ü–æ–≤—ñ–ª—å–Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—è

1. –ó–±—ñ–ª—å—à—Ç–µ `MODEL_THREADS` –≤ .env (–¥–æ –∫—ñ–ª—å–∫–æ—Å—Ç—ñ CPU cores)
2. –ó–º–µ–Ω—à—Ç–µ `MODEL_CONTEXT_SIZE` —è–∫—â–æ –Ω–µ –ø–æ—Ç—Ä—ñ–±–µ–Ω –≤–µ–ª–∏–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
3. –í–∏–∫–æ—Ä–∏—Å—Ç–∞–π—Ç–µ Q4_K_M –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü—ñ—é –∑–∞–º—ñ—Å—Ç—å Q5_K_M

### Out of Memory

1. –ó–º–µ–Ω—à—Ç–µ `MAX_CONCURRENT_REQUESTS`
2. –í–∏–∫–æ—Ä–∏—Å—Ç–∞–π—Ç–µ –º–µ–Ω—à—É –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü—ñ—é (Q4 –∑–∞–º—ñ—Å—Ç—å Q5)
3. –ó–º–µ–Ω—à—Ç–µ `MODEL_CONTEXT_SIZE`

## –õ—ñ—Ü–µ–Ω–∑—ñ—è

–¶–µ –ø—Ä–æ–µ–∫—Ç –ø—ñ–¥–ø–∞–¥–∞—î –ø—ñ–¥ Gemma Terms of Use –≤—ñ–¥ Google.

## –ü—ñ–¥—Ç—Ä–∏–º–∫–∞

–î–ª—è –ø–∏—Ç–∞–Ω—å —Ç–∞ issues —Å—Ç–≤–æ—Ä—é–π—Ç–µ issue –≤ —Ü—å–æ–º—É —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä—ñ—ó.
